{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "5ade32a9-e346-448e-8cf3-68a0e93fa96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "26382c06-486b-497e-b887-cf56de84fb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping pages 1...\n",
      "Scraping pages 2...\n",
      "Scraping pages 3...\n",
      "Scraping pages 4...\n",
      "Scraping pages 5...\n",
      "Scraping pages 6...\n",
      "Scraping pages 7...\n",
      "Scraping pages 8...\n",
      "Scraping pages 9...\n",
      "Scraping pages 10...\n",
      "Scraping pages 11...\n",
      "Scraping pages 12...\n",
      "Scraping pages 13...\n",
      "Scraping pages 14...\n",
      "Scraping pages 15...\n",
      "Scraping pages 16...\n",
      "Scraping pages 17...\n",
      "Scraping pages 18...\n",
      "Scraping pages 19...\n",
      "Scraping pages 20...\n",
      "Scraping pages 21...\n",
      "Scraping pages 22...\n",
      "Scraping pages 23...\n",
      "Scraping pages 24...\n",
      "Scraping pages 25...\n",
      "Scraping pages 26...\n",
      "Scraping pages 27...\n",
      "Scraping pages 28...\n",
      "Scraping pages 29...\n",
      "Scraping pages 30...\n",
      "Scraping pages 31...\n",
      "Scraping pages 32...\n",
      "Scraping pages 33...\n",
      "Scraping pages 34...\n",
      "Scraping pages 35...\n",
      "Scraping pages 36...\n",
      "Scraping pages 37...\n",
      "Scraping pages 38...\n",
      "Scraping pages 39...\n",
      "Scraping pages 40...\n",
      "Scraping pages 41...\n",
      "Scraping pages 42...\n",
      "Scraping pages 43...\n",
      "Scraping pages 44...\n",
      "Scraping pages 45...\n",
      "Scraping pages 46...\n",
      "Scraping pages 47...\n",
      "Scraping pages 48...\n",
      "Scraping pages 49...\n",
      "Scraping pages 50...\n",
      "Data saved.\n",
      "Scraping pages 51...\n",
      "Scraping pages 52...\n",
      "Scraping pages 53...\n",
      "Scraping pages 54...\n",
      "Scraping pages 55...\n",
      "Scraping pages 56...\n",
      "Scraping pages 57...\n",
      "Scraping pages 58...\n",
      "Scraping pages 59...\n",
      "Scraping pages 60...\n",
      "Scraping pages 61...\n",
      "Scraping pages 62...\n",
      "Scraping pages 63...\n",
      "Scraping pages 64...\n",
      "Scraping pages 65...\n",
      "Scraping pages 66...\n",
      "Scraping pages 67...\n",
      "Scraping pages 68...\n",
      "Scraping pages 69...\n",
      "Scraping pages 70...\n",
      "Scraping pages 71...\n",
      "Scraping pages 72...\n",
      "Scraping pages 73...\n",
      "Scraping pages 74...\n",
      "Scraping pages 75...\n",
      "Scraping pages 76...\n",
      "Scraping pages 77...\n",
      "Scraping pages 78...\n",
      "Scraping pages 79...\n",
      "Scraping pages 80...\n",
      "Data saved.\n",
      "Scraping and writing to Google Sheet completed.\n"
     ]
    }
   ],
   "source": [
    "def authenticate_google_sheet(json_path):\n",
    "    # Authenticate using the service account JSON file\n",
    "    creds = Credentials.from_service_account_file(json_path, scopes=[\n",
    "        'https://www.googleapis.com/auth/spreadsheets',\n",
    "        'https://www.googleapis.com/auth/drive'\n",
    "    ])\n",
    "    client = gspread.authorize(creds)\n",
    "    return client\n",
    "\n",
    "def scrape_yellowpages_to_sheet(base_url, worksheet, start_page=1, end_page=80, batch_size=50):\n",
    "    data = [[\"Name\", \"Location\", \"City\", \"P.O. Box\", \"Phone\", \"Mobile\", \"Company Page Link\", \"Logo URL\"]]\n",
    "    for page_number in range(start_page, end_page + 1):\n",
    "        page_url = urljoin(base_url, f'?page={page_number}')\n",
    "        print(f\"Scraping pages {page_number}...\")\n",
    "\n",
    "        page_response = requests.get(page_url)\n",
    "        if page_response.status_code == 200:\n",
    "            page_soup = BeautifulSoup(page_response.content, \"html.parser\")\n",
    "            listings = page_soup.find_all('div', class_='row box foc')\n",
    "\n",
    "            for listing in listings:\n",
    "                name_elem = listing.find('h2', class_='cmp_name')\n",
    "                name = name_elem.text.strip() if name_elem else \"N/A\"\n",
    "\n",
    "                location_elem = listing.find('span', itemprop='streetAddress')\n",
    "                location = location_elem.text.strip() if location_elem else \"N/A\"\n",
    "\n",
    "                city_elem = listing.find('strong', itemprop='addressLocality')\n",
    "                city = city_elem.text.strip() if city_elem else \"N/A\"\n",
    "\n",
    "                pobox_elem = listing.find('span', class_='pobox')\n",
    "                pobox = pobox_elem.text.strip() if pobox_elem else \"N/A\"\n",
    "\n",
    "                phone_elems = listing.find('span', class_='phonespn')\n",
    "                if phone_elems:\n",
    "                    phone_elems = phone_elems.find_all('span', class_='phone')\n",
    "                    phone = phone_elems[0].text.strip() if phone_elems else \"N/A\"\n",
    "                    mobile = phone_elems[1].text.strip() if len(phone_elems) > 1 else \"N/A\"\n",
    "                else:\n",
    "                    phone = \"N/A\"\n",
    "                    mobile = \"N/A\"\n",
    "\n",
    "                company_page_link_elem = listing.find('a', title='Restaurant suppliers in UAE')\n",
    "                company_page_link = urljoin(base_url, company_page_link_elem['href']) if company_page_link_elem else \"N/A\"\n",
    "\n",
    "                logo_url_elem = listing.find('img', itemprop='image')\n",
    "                logo_url = logo_url_elem.get('data-src') if logo_url_elem else \"N/A\"\n",
    "\n",
    "                data.append([name, location, city, pobox, phone, mobile, company_page_link, logo_url])\n",
    "\n",
    "            if page_number % batch_size == 0:\n",
    "                write_to_sheet(worksheet, data)\n",
    "                data = [[\"Name\", \"Location\", \"City\", \"P.O. Box\", \"Phone\", \"Mobile\", \"Company Page Link\", \"Logo URL\"]]\n",
    "            time.sleep(1)  # Adjust the delay as needed\n",
    "        else:\n",
    "            print(f\"Failed to retrieve page {page_number}.\")\n",
    "\n",
    "    # Write remaining data\n",
    "    if len(data) > 1:\n",
    "        write_to_sheet(worksheet, data)\n",
    "    print(\"Scraping and writing to Google Sheet completed.\")\n",
    "\n",
    "def write_to_sheet(worksheet, data):\n",
    "    worksheet.append_rows(data)\n",
    "    print(\"Data saved.\")\n",
    "\n",
    "# Specify the path to your service account JSON file\n",
    "json_path = \"C:\\\\Users\\\\HP PC\\\\Downloads\\\\magicpitch-llc-beed8422083c.json\"\n",
    "\n",
    "# Authenticate Google Sheet\n",
    "client = authenticate_google_sheet(json_path)\n",
    "\n",
    "# Open the Google Sheet by its title\n",
    "sheet = client.open(\"Python Scripts\")\n",
    "\n",
    "# Select the worksheet where you want to write data\n",
    "worksheet = sheet.worksheet(\"Sheet1\")\n",
    "\n",
    "# Define the base URL\n",
    "base_url = \"https://www.yellowpages-uae.com/uae/restaurant/\"\n",
    "\n",
    "# Scrape Yellow Pages and write to Google Sheet\n",
    "scrape_yellowpages_to_sheet(base_url, worksheet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb89d6a9-b57d-4cd8-8a58-f28604184082",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
